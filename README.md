# Distributed File System

### Usage

1. Go to repository root directory
2. Open terminal in this directory and run **python run super_client.py**
3. Place files to upload in **client_files** folder

Use following commands to work with DFS:

_(all file names should be related to **client_files** folder)_

- ***init*** initializes the server, discards all changes
- ***touch <file_name>*** creates an empty file
- ***w <file_name>*** uploads the file from **client_files** folder to the server
- ***r <file_name>*** reads the file from the server to **client_files** folder
- ***rm <file_name>*** removes the file
- ***info <file_name>*** gets information about the file from the server
- ***cp <source_file_name> <destination_file_name>*** creates copy of the file
- ***mv <source_path> <destination_path>*** moves a file from source path to destination path
- ***mv <source_path> <destination_path>*** moves a file from source path to destination path
- ***cd <new_directory>*** changes directory
- ***ls*** reads content of the directory
- ***mkdir <directory_name>*** creates new directory
- ***rmdir <directory_name> r(optional)*** remove directory _(r - remove recursively)_
- ***exit*** closes the client

### Architecture

![Architecture Diagram](https://github.com/ElBatanony/DFS/blob/master/ArchitectureDiagram.png)

There are three actors: super client, naming server, storage server.

***Naming server*** stores all information about directories and files organization. It stores file name and its info (e.g. file size) and creates UUID for each file in the system.

***Storage server*** stores all files in its root directory and sets their names to UUID generated by server.

Example of writing:

***Super client*** sends command to write a file to ***naming server*** and provides its name. ***Naming server*** then checks given file name on errors, adds it to the queue with generated UUID for it and sends UUID back to ***super client***. ***Super client*** then sends file's UUID and its content to ***storage server***.

### Communication protocols

All data is transfered via TCP protocol.

On AWS

***Used:***
- EC2 instance with Ubuntu
- Autoscaling group for supporting fixed amount of nodes
- VPC

Launch Template for Autoscaling group.

***The script:***

#!/bin/bash
sudo apt update
sudo apt install -y docker.io
sudo docker pull elbatanony/dfs (https://hub.docker.com/repository/docker/elbatanony/dfs):latest
sudo docker run -p 8800:8800 -p 8801:8801 -it elbatanony/dfs (https://hub.docker.com/repository/docker/elbatanony/dfs):latest


***VPC:***

Should be used the same for all instances, involved in DFS.


What happens inside of instance, when it starts?

It starts the script, that were stated as User Data, while we created an instance.
Script installs ubuntu and runs a container with image with our DFS.
